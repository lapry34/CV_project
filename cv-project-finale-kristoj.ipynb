{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2454327,"sourceType":"datasetVersion","datasetId":1485541},{"sourceId":3209332,"sourceType":"datasetVersion","datasetId":1946896}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{"id":"L_5SA5enwITK"}},{"cell_type":"code","source":"# Imports\n########################\n\n# Standard libraries\nimport os\nimport time\nimport random\nfrom collections import OrderedDict\n\nIS_COLAB = any(var.startswith(\"COLAB_\") for var in os.environ)\nIS_KAGGLE = any(var.startswith(\"KAGGLE_\") for var in os.environ)\n\nif IS_KAGGLE:\n  IS_COLAB = False\n  print(\"Running in Kaggle Notebook...\")\n  print(\"Installing libraries...\")\n  os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n  %pip install lpips --no-deps -q\n  %pip install wandb seaborn -qU\n  print(\"Libraries installed.\")\n\nif IS_COLAB:\n  print(\"Running in Google Colab...\")\n  print(\"Installing libraries...\")\n  os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n  %pip install lpips --no-deps -q\n  %pip install wandb seaborn -qU\n  print(\"Libraries installed.\")\n\nelse:\n  print(\"Not running in Google Colab.\")\n\n# Data manipulation and analysis\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import roc_auc_score, roc_curve, auc, precision_recall_curve\nfrom sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n\n# Image processing and visualization\nimport seaborn as sns\nimport skimage.io as io\nfrom skimage import measure\nfrom skimage.io import imread\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage import gaussian_filter\n\n# PyTorch and deep learning\nimport torch\nimport torch.nn as nn\nfrom lpips import LPIPS\nfrom torch.optim import Adam\nimport torch.utils.data as data\nimport torch.nn.functional as F\nimport torchvision.utils as utils\nfrom torchvision import transforms\nfrom torch.autograd import Variable\nfrom einops import rearrange, repeat\n\n# Environment\nimport wandb\nimport kagglehub\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-26T21:20:42.667415Z","iopub.execute_input":"2025-05-26T21:20:42.667712Z","iopub.status.idle":"2025-05-26T21:20:47.562992Z","shell.execute_reply.started":"2025-05-26T21:20:42.667694Z","shell.execute_reply":"2025-05-26T21:20:47.562172Z"},"id":"QpTDTGpAwITL","outputId":"45a227e8-6771-4244-bfbe-1646db6b7c8d","trusted":true},"outputs":[{"name":"stdout","text":"Running in Kaggle Notebook...\nInstalling libraries...\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\nLibraries installed.\nNot running in Google Colab.\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"# Setup","metadata":{"id":"jhbphjJiwITI"}},{"cell_type":"markdown","source":"## Drive Setup","metadata":{}},{"cell_type":"code","source":"# Google Drive Setup\n########################\n\n\n# Set up directories\nif IS_KAGGLE:\n    BASE_PATH = '/kaggle/working'\n    MODEL_SAVE_DIR = os.path.join(BASE_PATH, 'trained_models')\n    DATASET_DIR = '/kaggle/input'\n    MVTEC_DATA_DIR = os.path.join(DATASET_DIR, 'mvtec-ad')\n    BTAD_DATA_DIR = os.path.join(DATASET_DIR, 'btad-beantech-anomaly-detection')\n\nelif IS_COLAB:\n    from google.colab import drive\n    print(\"Mounting Google Drive...\")\n    drive.mount('/content/drive')\n    print(\"Google Drive mounted.\")\n    BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/Computer Vision/'\n    MODEL_SAVE_DIR = os.path.join(BASE_PATH, 'trained_models')\n    DATASET_DIR = os.path.join(BASE_PATH, 'datasets')\n    MVTEC_DATA_DIR = os.path.join(DATASET_DIR, 'mvtec')\n    BTAD_DATA_DIR = os.path.join(DATASET_DIR, 'btad')\nelse:\n    BASE_PATH = os.path.dirname(os.curdir) #os.path.abspath('.')\n    MODEL_SAVE_DIR = os.path.join(BASE_PATH, 'trained_models')\n    DATASET_DIR = os.path.join(BASE_PATH, 'datasets')\n\n# Download MVTEC and BTAD dataset\n# MVTEC_DATA_DIR = kagglehub.dataset_download(\"ipythonx/mvtec-ad\", force_download=False)\n# BTAD_DATA_DIR = kagglehub.dataset_download(\"thtuan/btad-beantech-anomaly-detection\", force_download=False)\n\n# Create directories if they don't exist\nos.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n# os.makedirs(MVTEC_DATA_DIR, exist_ok=True)\n# os.makedirs(BTAD_DATA_DIR, exist_ok=True)\n\n# Set num_workers for data loaders\nNUM_WORKERS_PARAM = {\n    'num_workers': 0  # Set to 0 for Colab to avoid crashes\n}\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-26T21:20:47.564491Z","iopub.execute_input":"2025-05-26T21:20:47.564741Z","iopub.status.idle":"2025-05-26T21:20:47.571500Z","shell.execute_reply.started":"2025-05-26T21:20:47.564719Z","shell.execute_reply":"2025-05-26T21:20:47.570819Z"},"id":"n8njAowNwITK","outputId":"fb0cca14-da34-455a-aced-368cbe91e8b9","trusted":true},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"## Device Setup","metadata":{}},{"cell_type":"code","source":"# Set up seeds\nSEED = 71\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\n\n# Set up device\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif DEVICE.type == 'cuda':\n    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.enabled = True\nelse:\n    print(\"CUDA is not available. Using CPU.\")\n    torch.backends.cudnn.enabled = False\n","metadata":{"jupyter":{"source_hidden":true},"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T21:20:47.572591Z","iopub.execute_input":"2025-05-26T21:20:47.572853Z","iopub.status.idle":"2025-05-26T21:20:47.602184Z","shell.execute_reply.started":"2025-05-26T21:20:47.572832Z","shell.execute_reply":"2025-05-26T21:20:47.601469Z"}},"outputs":[{"name":"stdout","text":"Using GPU: Tesla T4\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"# Globals","metadata":{"id":"fG1p3D4cwITL"}},{"cell_type":"code","source":"# Globals\n########################\n\n####################\n# Script Arguments #\n####################\nTRAIN_MODEL = True      # True: train+test, False: test\nUSING_MVTEC = False\nNUM_EPOCHS = 400\nUSING_DYT = True\nLOG_LPIPS = False\nUSING_LW_SSIM = True\nUSING_RANDOM_MASK = True\n\nCOMPARE_MODELS = False #TODO?\n\n# Model Configuration Parameters\nBATCH_SIZE = 8\nPATCH_SIZE = 64\nNUM_GAUSSIANS = 150\nLEARNING_RATE = 1e-4\n\nLAMBDA_MSE = 5\nDYT_INIT_A = 0.5\nLAMBDA_LPIPS = 0        # we only log it\nLAMBDA_SSIM = 0.5\nNOISE_DECAY = True\nWEIGHT_DECAY = 1e-4\nNOISE_DECAY_FACTOR = 1  # handles noise decaying, better not touch\n\n\n# --- Derived Configurations ---\nNORM_STR = \"DyT\" if USING_DYT else \"LayerNorm\"\nDATASET_STR = \"MVTEC\" if USING_MVTEC else \"BTAD\"\nDATASET_CAT_STR = \"bottle\" if USING_MVTEC else '01' # Example: 'bottle', 'cable', 'hazelnut' for MVTec; '01', '02', '03' for BTAD\nRUN_NAME = f\"{NORM_STR}_{DATASET_STR}_e{NUM_EPOCHS}_\" + (\"lwssim\" if USING_LW_SSIM else \"ssim\") + \"_\" + (\"randommask\" if USING_RANDOM_MASK else \"no_randommask\") + \"_\" + (\"kaggle\" if IS_KAGGLE else \"colab\")\n\nmodel_path = os.path.join(MODEL_SAVE_DIR, f'VT_AE_{NORM_STR}_{DATASET_STR}_{DATASET_CAT_STR}.pt')\ng_path = os.path.join(MODEL_SAVE_DIR, f'G_estimate_{NORM_STR}_{DATASET_STR}_{DATASET_CAT_STR}.pt')\n\n\n# Minimum number of patches for Vision Transformer\nMIN_NUM_PATCHES = 16\n\n# Visualization settings\nPLOT_COLORS = {\"normal\": \"green\", \"anomaly\": \"red\"}\nPLOT_STYLE = \"whitegrid\"\n\n# Time measurements\nTIME_IN_NORM = 0\nNORM_CALLS = 0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-26T21:20:47.603784Z","iopub.execute_input":"2025-05-26T21:20:47.604449Z","iopub.status.idle":"2025-05-26T21:20:47.627572Z","shell.execute_reply.started":"2025-05-26T21:20:47.604410Z","shell.execute_reply":"2025-05-26T21:20:47.626840Z"},"id":"k4h2FDzAwITL","outputId":"4577fd3d-da79-4850-d4ca-30b8cf8d9ef7","trusted":true},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"# Utils","metadata":{"id":"6cNgivo9wITL"}},{"cell_type":"markdown","source":"## Weight & Biases ","metadata":{}},{"cell_type":"code","source":"def setup_wandb():\n    \"\"\"\n    Setup wandb for logging\n    \"\"\"\n\n    # Get the wandb API key from secrets or environment variables\n    if IS_COLAB:\n        from google.colab import userdata\n        wandbkey = userdata.get('WANDB_API_KEY')\n\n    elif IS_KAGGLE:\n        from kaggle_secrets import UserSecretsClient\n        user_secrets = UserSecretsClient()\n        wandbkey = user_secrets.get_secret(\"WANDB_API_KEY\")\n\n    else:\n        wandbkey = os.environ['WANDB_API_KEY']\n\n    # Initialize wandb\n    wandb.login(key=wandbkey)\n    wandb.init(\n            entity = \"CV_albe_gab_kri\",\n            project = \"CV_Final_runs\",\n            name = RUN_NAME,\n            config={\n                # Dataset Configuration\n                \"dataset\": DATASET_STR,\n                \"product_category\": DATASET_CAT_STR,\n                \"using_mvtec\": USING_MVTEC,\n\n                # Model Architecture\n                \"patch_size\": PATCH_SIZE,\n\n                # Training Hyperparameters\n                \"num_epochs\": NUM_EPOCHS,\n                \"learning_rate\": LEARNING_RATE,\n                \"weight_decay\": WEIGHT_DECAY,\n                \"batch_size\": BATCH_SIZE,\n\n                # Loss Function Weights\n                \"lambda_mse\": LAMBDA_MSE,\n                \"lambda_ssim\": LAMBDA_SSIM,\n\n                # Normalization and Activation\n                \"normalization_type\": NORM_STR,\n                \"dyt_init_a\": DYT_INIT_A,\n\n                # Loss Function Configuration\n                \"using_lw_ssim\": USING_LW_SSIM,\n                \"ssim_type\": \"LWSSIM\" if USING_LW_SSIM else \"Standard SSIM\",\n\n                # Data Augmentation and Noise\n                \"using_random_mask\": USING_RANDOM_MASK,\n                \"noise_decay\": NOISE_DECAY,\n                \"noise_decay_factor\": NOISE_DECAY_FACTOR,\n\n                # Model Paths for Reproducibility\n                \"base_path\": BASE_PATH,\n                \"model_save_dir\": MODEL_SAVE_DIR,\n                \"dataset_dir\": DATASET_DIR,\n\n                # Hardware Configuration\n                \"device\": str(DEVICE),\n                \"seed\": SEED,\n\n                # Environment Detection\n                \"computed_by\" : \"COLAB\" if IS_COLAB else (\"KAGGLE\" if IS_KAGGLE else \"LOCAL\"),\n\n                # Training Configuration Flags\n                \"train_model\": TRAIN_MODEL,\n                \"compare_models\": COMPARE_MODELS,\n            })\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T21:20:47.628339Z","iopub.execute_input":"2025-05-26T21:20:47.628648Z","iopub.status.idle":"2025-05-26T21:20:47.646540Z","shell.execute_reply.started":"2025-05-26T21:20:47.628630Z","shell.execute_reply":"2025-05-26T21:20:47.645897Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"## Plots","metadata":{}},{"cell_type":"code","source":"def plot(image, grnd_truth, score):\n    \"\"\"Plot image, ground truth and predicted score\"\"\"\n    plt.figure(figsize=(15, 5))\n    plt.subplot(131)\n    plt.imshow(image[0].permute(1, 2, 0))\n    plt.title('Original Image')\n    plt.subplot(132)\n    plt.imshow(grnd_truth.squeeze(0).squeeze(0))\n    plt.title('Ground Truth')\n    plt.subplot(133)\n    plt.imshow(score)\n    plt.title('Anomaly Score')\n    plt.colorbar()\n    plt.tight_layout()\n    plt.pause(1)\n    plt.show()\n\ndef plot_enhanced(image, grnd_truth, reconstructed, score, threshold=None, save_path=None):\n    \"\"\"Enhanced plot with threshold and better visualization\"\"\"\n    with sns.plotting_context(\"talk\"):\n        sns.set_style(PLOT_STYLE)\n        plt.figure(figsize=(24, 8))\n\n        # Original image\n        plt.subplot(151)\n        plt.imshow(image[0].permute(1, 2, 0))\n        plt.title('Original Image')\n        plt.axis('off')\n\n        plt.subplot(152)\n        plt.imshow(reconstructed[0].permute(1, 2, 0).clip(0,1))\n        plt.title('Reconstructed Image')\n        plt.axis('off')\n\n        # Ground truth\n        plt.subplot(153)\n        plt.imshow(grnd_truth.squeeze(0).squeeze(0), cmap='gray')\n        plt.title('Ground Truth')\n        plt.axis('off')\n\n        # Anomaly score\n        plt.subplot(154)\n        heatmap = plt.imshow(score, cmap='jet')\n        plt.title('Anomaly Score')\n        plt.axis('off')\n        plt.colorbar(heatmap, fraction=0.046, pad=0.04)\n\n        # Thresholded result (if threshold provided)\n        if threshold is not None:\n            plt.subplot(155)\n            binary_mask = np.where(score > threshold, 1., 0.)\n            plt.imshow(binary_mask, cmap='gray')\n            plt.title(f'Binary Result (t={threshold:.3f})')\n            plt.axis('off')\n\n        plt.tight_layout()\n\n        if save_path:\n            plt.savefig(save_path, bbox_inches='tight')\n        plt.show()\n\ndef plot_roc_curve(fpr, tpr, auc_score, color='darkorange', label='', title='Receiver Operating Characteristic', save_path=None):\n    \"\"\"Plot ROC curve with AUC score\"\"\"\n    with sns.plotting_context(\"talk\"):\n        sns.set_style(PLOT_STYLE)\n        plt.figure(figsize=(8, 8))\n        plt.plot(fpr, tpr, lw=2, color=color, label=f'(AUC = {auc_score:.3f})')\n        plt.fill_between(fpr, tpr, alpha=0.2, color=color)\n        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title(title)\n        plt.legend(loc=\"lower right\")\n        if save_path:\n            plt.savefig(save_path, bbox_inches='tight')\n        plt.show()\n\ndef plot_precision_recall_curve(precision, recall, pr_auc, color='green', label='', title='Precision-Recall Curve', save_path=None):\n    \"\"\"Plot precision-recall curve with AUC score\"\"\"\n    with sns.plotting_context(\"talk\"):\n        sns.set_style(PLOT_STYLE)\n        plt.figure(figsize=(8, 8))\n        plt.plot(recall, precision, lw=2, color=color, label=f'(AUC = {pr_auc:.3f})')\n        plt.fill_between(recall, precision, alpha=0.2, color=color)\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('Recall')\n        plt.ylabel('Precision')\n        plt.title(title)\n        plt.legend(loc=\"lower left\")\n        if save_path:\n            plt.savefig(save_path, bbox_inches='tight')\n        plt.show()\n\ndef plot_confusion_matrix(y_true, y_pred, save_path=None, normalize=False):\n    \"\"\"Plot (optionally normalized) confusion matrix using seaborn\"\"\"\n    with sns.plotting_context(\"talk\"):\n        sns.set_style(PLOT_STYLE)\n        cm = confusion_matrix(y_true, y_pred)\n\n        if normalize:\n            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n        plt.figure(figsize=(8, 8))\n        sns.heatmap(cm, annot=True, fmt='.2f' if normalize else 'd', cmap='Blues',\n                    xticklabels=['Normal', 'Anomaly'],\n                    yticklabels=['Normal', 'Anomaly'])\n        plt.xlabel('Predicted')\n        plt.ylabel('True')\n        plt.title('Confusion Matrix' + (' (Normalized)' if normalize else ''))\n        if save_path:\n            plt.savefig(save_path, bbox_inches='tight')\n        plt.show()\n\ndef plot_score_distributions(normal_scores, anomaly_scores, threshold=None, save_path=None):\n    \"\"\"Plot histogram of normal and anomaly scores with threshold line if provided\"\"\"\n    with sns.plotting_context(\"talk\"):\n        sns.set_style(PLOT_STYLE)\n        plt.figure(figsize=(10, 6))\n        sns.histplot(normal_scores, color=PLOT_COLORS[\"normal\"], label=\"Normal\", alpha=0.6, kde=True)\n        sns.histplot(anomaly_scores, color=PLOT_COLORS[\"anomaly\"], label=\"Anomaly\", alpha=0.6, kde=True)\n\n        if threshold is not None:\n            plt.axvline(x=threshold, color='black', linestyle='--', label=f'Threshold: {threshold:.3f}')\n\n        plt.xlabel('Anomaly Score')\n        plt.ylabel('Density')\n        plt.title('Distribution of Anomaly Scores')\n        plt.legend()\n\n        if save_path:\n            plt.savefig(save_path, bbox_inches='tight')\n        plt.show()\n\ndef visualize_regions(image, score_map, threshold, min_area=100, save_path=None):\n    \"\"\"Visualize detected anomaly regions\"\"\"\n    # Threshold the score map and find contours\n    binary_mask = np.where(score_map > threshold, 1., 0.)\n    labeled_mask = measure.label(binary_mask)\n    regions = measure.regionprops(labeled_mask)\n\n    # Filter small regions\n    filtered_regions = [region for region in regions if region.area >= min_area]\n\n    # Visualize\n    with sns.plotting_context(\"talk\"):\n        sns.set_style(PLOT_STYLE)\n        plt.figure(figsize=(12, 8))\n\n        # Original image\n        plt.subplot(121)\n        plt.imshow(image[0].permute(1, 2, 0))\n        plt.title('Original Image')\n        plt.axis('off')\n\n        # Image with detected regions\n        plt.subplot(122)\n        plt.imshow(image[0].permute(1, 2, 0))\n\n        # Draw bounding boxes around regions\n        for region in filtered_regions:\n            minr, minc, maxr, maxc = region.bbox\n            rect = plt.Rectangle((minc, minr), maxc - minc, maxr - minr,\n                                fill=False, edgecolor='red', linewidth=2)\n            plt.gca().add_patch(rect)\n\n            # Add area text\n            plt.text(minc, minr - 5, f\"Area: {region.area}\",\n                    color='white', fontsize=9, backgroundcolor='red')\n\n        plt.title(f'Detected Anomalies (n={len(filtered_regions)})')\n        plt.axis('off')\n\n        plt.tight_layout()\n        if save_path:\n            plt.savefig(save_path, bbox_inches='tight')\n        plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2025-05-26T21:20:47.647158Z","iopub.execute_input":"2025-05-26T21:20:47.647311Z","iopub.status.idle":"2025-05-26T21:20:47.678165Z","shell.execute_reply.started":"2025-05-26T21:20:47.647298Z","shell.execute_reply":"2025-05-26T21:20:47.677258Z"},"id":"VdKRCL5RwITM","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"## Newtork Utils","metadata":{}},{"cell_type":"code","source":"def initialize_weights(*models):\n    \"\"\"Initialize network weights using kaiming normal\"\"\"\n    for model in models:\n        for module in model.modules():\n            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n                nn.init.kaiming_normal_(module.weight)\n                if module.bias is not None:\n                    module.bias.data.zero_()\n            elif isinstance(module, nn.BatchNorm2d):\n                module.weight.data.fill_(1)\n                module.bias.data.zero_()\n\n\ndef process_mask(mask):\n    \"\"\"Process mask to binary format\"\"\"\n    mask = np.where(mask > 0., 1, mask)\n    return torch.tensor(mask)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T21:20:47.679015Z","iopub.execute_input":"2025-05-26T21:20:47.679628Z","iopub.status.idle":"2025-05-26T21:20:47.703156Z","shell.execute_reply.started":"2025-05-26T21:20:47.679607Z","shell.execute_reply":"2025-05-26T21:20:47.702447Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"# Datasets","metadata":{"id":"pokDqDWwwITM"}},{"cell_type":"markdown","source":"## MVTEC","metadata":{"id":"GkPiV2lI4E8D"}},{"cell_type":"code","source":"class MVTEC:\n    def __init__(self, batch_size, root=MVTEC_DATA_DIR, product='bottle'):\n        self.root = root\n        self.batch = batch_size\n        self.product = product\n\n        if self.product == 'all':\n            print('--------Please select a valid product.......See Train_data function-----------')\n        else:\n            # Importing all the image_path dictionaries for test and train data\n            train_path_images = self.Train_data()\n            test_norm_path_images = self.Test_normal_data()\n            test_anom_image_paths, test_anom_mask_paths = self.load_test_anom_images_and_masks()\n\n            # Image Transformation\n            T = transforms.Compose([\n                transforms.ToPILImage(),\n                transforms.Resize((550, 550)),\n                transforms.CenterCrop(512),\n                transforms.ToTensor(),\n            ])\n\n            train_normal_image = torch.stack([T(self.load_images(j, i)) for j in train_path_images.keys() for i in train_path_images[j]])\n            test_normal_image = torch.stack([T(self.load_images(j, i)) for j in test_norm_path_images.keys() for i in test_norm_path_images[j]])\n\n            train_normal_mask = torch.zeros(train_normal_image.size(0), 1, train_normal_image.size(2), train_normal_image.size(3))\n            test_normal_mask = torch.zeros(test_normal_image.size(0), 1, test_normal_image.size(2), test_normal_image.size(3))\n\n            test_anom_image = torch.stack([\n                T(self.load_images(os.path.dirname(p), os.path.basename(p)))\n                for p in test_anom_image_paths\n            ])\n            \n            test_anom_mask = torch.stack([\n                process_mask(T(self.load_images(os.path.dirname(p), os.path.basename(p))))\n                for p in test_anom_mask_paths\n            ])\n\n            \n            train_normal = tuple(zip(train_normal_image, train_normal_mask))\n            test_anom = tuple(zip(test_anom_image, test_anom_mask))\n            test_normal = tuple(zip(test_normal_image, test_normal_mask))\n\n            print(f' --Size of {self.product} train loader: {train_normal_image.size()}--')\n            if test_anom_image.size(0) == test_anom_mask.size(0):\n                print(f' --Size of {self.product} test anomaly loader: {test_anom_image.size()}--')\n            else:\n                print(f'[!Info] Size Mismatch between Anomaly images {test_anom_image.size()} and Masks {test_anom_mask.size()} Loaded')\n            print(f' --Size of {self.product} test normal loader: {test_normal_image.size()}--')\n\n            # Create validation set\n            num = self.ran_generator(len(test_anom), 10)\n            val_anom = [test_anom[i] for i in num]\n            num = self.ran_generator(len(test_normal), 10)\n            val_norm = [test_normal[j] for j in num]\n            val_set = [*val_norm, *val_anom]\n            print(f' --Total Image in {self.product} Validation loader: {len(val_set)}--')\n\n            # Final Data Loader - Updated with num_workers=0\n            self.train_loader = torch.utils.data.DataLoader(train_normal, batch_size=batch_size, shuffle=True, **NUM_WORKERS_PARAM)\n            self.test_anom_loader = torch.utils.data.DataLoader(test_anom, batch_size=batch_size, shuffle=False, **NUM_WORKERS_PARAM)\n            self.test_norm_loader = torch.utils.data.DataLoader(test_normal, batch_size=batch_size, shuffle=False, **NUM_WORKERS_PARAM)\n            self.validation_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False, **NUM_WORKERS_PARAM)\n\n    def ran_generator(self, length, shots=1):\n        \"\"\"Generate random indices\"\"\"\n        rand_list = random.sample(range(0, length), shots)\n        return rand_list\n\n    def read_files(self, d, data_motive='train', use_good=True, normal=True):\n        '''\n        Return the path of the train directory and list of train images\n\n        Parameters:\n            root : root directory of mvtech images\n            d = List of directories in the root directory\n            product : name of the product to return the images for single class training\n            data_motive : Can be 'train' or 'test' or 'ground_truth'\n            use_good : To use the data in the good folder\n            normal : Signify if the normal images are included\n\n        Returns:\n            Path and Image ordered dict for the dataset\n        '''\n        files = next(os.walk(os.path.join(self.root, d)))[1]\n        for d_in in files:\n            if os.path.isdir(os.path.join(self.root, d, d_in)):\n                if d_in == data_motive:\n                    im_pt = OrderedDict()\n                    file = os.listdir(os.path.join(self.root, d, d_in))\n\n                    for i in file:\n                        if os.path.isdir(os.path.join(self.root, d, d_in, i)):\n                            if (data_motive == 'train'):\n                                tr_img_pth = os.path.join(self.root, d, d_in, i)\n                                images = os.listdir(tr_img_pth)\n                                im_pt[tr_img_pth] = images\n                                print(f'total {d_in} images of {i} {d} are: {len(images)}')\n\n                            if (data_motive == 'test'):\n                                if (use_good == False) and (i == 'good') and normal != True:\n                                    print(f'the good images for {d_in} images of {i} {d} is not included in the test anomalous data')\n                                elif (use_good == False) and (i != 'good') and normal != True:\n                                    tr_img_pth = os.path.join(self.root, d, d_in, i)\n                                    images = os.listdir(tr_img_pth)\n                                    im_pt[tr_img_pth] = images\n                                    print(f'total {d_in} images of {i} {d} are: {len(images)}')\n                                elif (use_good == True) and (i == 'good') and (normal == True):\n                                    tr_img_pth = os.path.join(self.root, d, d_in, i)\n                                    images = os.listdir(tr_img_pth)\n                                    im_pt[tr_img_pth] = images\n                                    print(f'total {d_in} images of {i} {d} are: {len(images)}')\n                            if (data_motive == 'ground_truth'):\n                                tr_img_pth = os.path.join(self.root, d, d_in, i)\n                                images = os.listdir(tr_img_pth)\n                                im_pt[tr_img_pth] = images\n                                print(f'total {d_in} images of {i} {d} are: {len(images)}')\n                    if self.product == \"all\":\n                        return\n                    else:\n                        return im_pt\n\n    def load_images(self, path, image_name):\n        \"\"\"Load image from path\"\"\"\n        return imread(os.path.join(path, image_name))\n\n    def Train_data(self, use_good=True):\n      '''\n      Return the path of the train directory and list of train images\n      '''\n      dir = os.listdir(self.root)\n\n      for d in dir:\n          if self.product == \"all\":\n              self.read_files(d, data_motive='train')\n          elif self.product == d:\n              pth_img = self.read_files( d, data_motive='train')\n              return pth_img\n\n    def load_test_anom_images_and_masks(self):\n        \"\"\"\n        Return two lists: paths of anomalous test images and their corresponding masks.\n        Assumes:\n          - image: '000.png'\n          - mask:  '000_mask.png'\n        \"\"\"\n        image_paths = []\n        mask_paths = []\n        dir = os.listdir(self.root)\n    \n        for d in dir:\n            if self.product != d:\n                continue\n    \n            # Anomalous images\n            test_dir = os.path.join(self.root, d, 'test')\n            gt_dir = os.path.join(self.root, d, 'ground_truth')\n    \n            for defect_type in os.listdir(test_dir):\n                if defect_type == 'good':\n                    continue\n    \n                defect_image_dir = os.path.join(test_dir, defect_type)\n                defect_mask_dir = os.path.join(gt_dir, defect_type)\n    \n                for img_file in os.listdir(defect_image_dir):\n                    img_path = os.path.join(defect_image_dir, img_file)\n                    mask_file = img_file.replace('.png', '_mask.png')\n                    mask_path = os.path.join(defect_mask_dir, mask_file)\n    \n                    if not os.path.exists(mask_path):\n                        print(f\"[!] Warning: No mask found for image {img_file}\")\n                        continue\n    \n                    image_paths.append(img_path)\n                    mask_paths.append(mask_path)\n    \n            break  # since we found the product, no need to continue\n    \n        return image_paths, mask_paths\n\n    \n    def Test_normal_data(self, use_good=True):\n        '''\n        Return path and images for normal test data\n        '''\n        if self.product == 'all':\n            print('Please choose a valid product. Normal test data can be seen product wise')\n            return\n        dir = os.listdir(self.root)\n\n        for d in dir:\n            if self.product == d:\n                pth_img = self.read_files(d, data_motive='test', use_good=True, normal=True)\n                return pth_img\n\n","metadata":{"id":"JmRGQ1lt4EMZ","trusted":true,"execution":{"iopub.status.busy":"2025-05-26T21:20:47.704032Z","iopub.execute_input":"2025-05-26T21:20:47.704863Z","iopub.status.idle":"2025-05-26T21:20:47.727134Z","shell.execute_reply.started":"2025-05-26T21:20:47.704845Z","shell.execute_reply":"2025-05-26T21:20:47.726344Z"}},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"## BTAD","metadata":{"id":"mcZbOybxt92h"}},{"cell_type":"code","source":"class BTAD:\n    \"\"\"\n    BTAD dataset class that mirrors the Mvtec class interface.\n    This ensures consistency between MVTec and BTAD dataset handling.\n    \"\"\"\n    def __init__(self, batch_size, root=BTAD_DATA_DIR, product='01'):\n        self.root = root\n        self.batch = batch_size\n        self.product = product\n\n        if self.product not in ['01', '02', '03']:\n            print(f'--------Please select a valid product: 01, 02, or 03. Got: {self.product}-----------')\n        else:\n            print(f\"Loading BTAD dataset for product {self.product}...\")\n\n            # Create individual DataLoaders using your existing load_btad_dataset function\n            # Training data (normal samples only)\n            self.train_loader = load_btad_dataset(\n                root_dir=self.root,\n                product=self.product,\n                batch_size=batch_size,\n                mode='train',\n                resize=True,\n                use_normal=True,\n                anomalous=False\n            )\n\n            # Test normal data (normal samples from test set)\n            self.test_norm_loader = load_btad_dataset(\n                root_dir=self.root,\n                product=self.product,\n                batch_size=batch_size,\n                mode='test',\n                resize=True,\n                use_normal=True,\n                anomalous=False\n            )\n\n            # Test anomalous data (anomalous samples from test set with ground truth masks)\n            self.test_anom_loader = load_btad_dataset(\n                root_dir=self.root,\n                product=self.product,\n                batch_size=batch_size,\n                mode='test',\n                resize=True,\n                use_normal=False,\n                anomalous=True\n            )\n\n            # Create a simple validation loader (you can enhance this later)\n            # For now, we'll use the test normal loader as validation\n            # In a more sophisticated setup, you might want to split the training data\n            self.validation_loader = self.test_norm_loader\n\n            print(f\"BTAD dataset for product {self.product} loaded successfully!\")\n            print(f\"Train loader: Ready\")\n            print(f\"Test normal loader: Ready\")\n            print(f\"Test anomalous loader: Ready\")\n            print(f\"Validation loader: Using test normal data\")\n\n\nclass BTADDataset(data.Dataset):\n    def __init__(self, images, masks=None, resize=True):\n        \"\"\"\n        BTAD dataset class that exactly matches MVTec's data structure\n\n        This version ensures that masks are processed identically to MVTec,\n        eliminating shape inconsistencies in downstream processing.\n        \"\"\"\n        self.images = images\n        self.masks = masks\n        self.resize = resize\n\n        self.resize_transform = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize((512, 512)),  # Match VT-ADL Paper\n            transforms.ToTensor()\n        ])\n\n        self.default_transform = transforms.Compose([transforms.ToTensor()])\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, i):\n        # Load and process image exactly like MVTec\n        if isinstance(self.images[i], str):\n            image_ = io.imread(self.images[i])\n\n            # Convert grayscale to RGB if needed\n            if len(image_.shape) < 3:\n                image_ = np.stack((image_,) * 3, axis=-1)\n\n            if self.resize:\n                image = self.resize_transform(image_)\n            else:\n                image = self.default_transform(image_)\n        else:\n            image = self.images[i]\n\n        # Process masks to EXACTLY match MVTec's structure\n        if self.masks is not None and self.masks[i] is not None:\n            if isinstance(self.masks[i], str):\n                # Load mask from file and apply SAME transform as image\n                mask_ = io.imread(self.masks[i], as_gray=True)\n\n                if self.resize:\n                    # Apply the EXACT same transform pipeline as the image\n                    # This ensures mask and image have identical dimensions\n                    mask = self.resize_transform(mask_)\n                else:\n                    mask = self.default_transform(mask_)\n\n                # Apply process_mask function exactly like MVTec does\n                mask = process_mask(mask)\n            else:\n                mask = self.masks[i]\n        else:\n            # Create zero mask with EXACT same structure as MVTec\n            # MVTec creates: torch.zeros(batch_size, 1, height, width)\n            # For individual samples, this becomes: torch.zeros(1, height, width)\n            mask = torch.zeros(1, image.size(1), image.size(2))\n\n        return image, mask\n\ndef load_btad_dataset(root_dir, product, batch_size, mode='train', resize=True, use_normal=True, anomalous=False):\n    \"\"\"\n    Load BTAD dataset with MVTec-compatible data structures.\n    Looks for the product folder, even if nested one level under root_dir.\n    \"\"\"\n    # Attempt direct path\n    product_path = os.path.join(root_dir, product)\n\n    # If not found, search one level deeper\n    if not os.path.isdir(product_path):\n        for sub in sorted(os.listdir(root_dir)):\n            sub_path = os.path.join(root_dir, sub)\n            candidate = os.path.join(sub_path, product)\n            if os.path.isdir(candidate):\n                print(f\"Found product '{product}' under nested directory '{sub}'.\")\n                root_dir = sub_path\n                product_path = candidate\n                break\n\n    # Final check\n    if not os.path.isdir(product_path):\n        available = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n        raise ValueError(f\"Product '{product}' not found in {root_dir}. Available products: {available}\")\n\n    image_paths = []\n    mask_paths = []\n\n    # Define paths\n    if mode == 'train':\n        base_img_dir = os.path.join(product_path, 'train')\n        gt_available = False\n    else:\n        base_img_dir = os.path.join(product_path, 'test')\n        gt_dir = os.path.join(product_path, 'ground_truth', 'ko')\n        gt_available = os.path.isdir(gt_dir)\n\n    # Load images\n    if mode == 'train':\n        # Normal samples\n        if use_normal:\n            ok_dir = os.path.join(base_img_dir, 'ok')\n            if os.path.isdir(ok_dir):\n                for f in sorted(os.listdir(ok_dir)):\n                    if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n                        image_paths.append(os.path.join(ok_dir, f))\n                        mask_paths.append(None)\n        # Anomalous training\n        if anomalous:\n            ko_dir = os.path.join(base_img_dir, 'ko')\n            if os.path.isdir(ko_dir):\n                for f in sorted(os.listdir(ko_dir)):\n                    if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n                        image_paths.append(os.path.join(ko_dir, f))\n                        mask_paths.append(None)\n    else:\n        # Test normal\n        if use_normal:\n            ok_dir = os.path.join(base_img_dir, 'ok')\n            if os.path.isdir(ok_dir):\n                for f in sorted(os.listdir(ok_dir)):\n                    if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n                        image_paths.append(os.path.join(ok_dir, f))\n                        mask_paths.append(None)\n        # Test anomalous\n        if anomalous:\n            ko_dir = os.path.join(base_img_dir, 'ko')\n            if os.path.isdir(ko_dir):\n                for f in sorted(os.listdir(ko_dir)):\n                    if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n                        image_paths.append(os.path.join(ko_dir, f))\n                        # ground truth mask\n                        mask_file_base = os.path.splitext(f)[0]\n                        mask = None\n                        if gt_available:\n                            for ext in ['.png', '.jpg', '.jpeg', '.bmp']:\n                                candidate = os.path.join(gt_dir, mask_file_base + ext)\n                                if os.path.isfile(candidate):\n                                    mask = candidate\n                                    break\n                        if mask is None:\n                            print(f\"Warning: No mask for {f}\")\n                        mask_paths.append(mask)\n\n    print(f\"Found {len(image_paths)} images for product {product} ({mode}, ok={use_normal}, ko={anomalous})\")\n\n    # Create dataset and dataloader\n    dataset = BTADDataset(image_paths, mask_paths, resize)\n    data_loader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=(mode == 'train'),\n        **NUM_WORKERS_PARAM\n    )\n    return data_loader\n","metadata":{"id":"zVG5YSYouAem","trusted":true,"execution":{"iopub.status.busy":"2025-05-26T21:20:47.728224Z","iopub.execute_input":"2025-05-26T21:20:47.728552Z","iopub.status.idle":"2025-05-26T21:20:47.757847Z","shell.execute_reply.started":"2025-05-26T21:20:47.728528Z","shell.execute_reply":"2025-05-26T21:20:47.757275Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"# Losses","metadata":{"id":"hYT6BrzEQJ_m"}},{"cell_type":"markdown","source":"## SSIM","metadata":{"id":"DZWGjAxlQe_m"}},{"cell_type":"code","source":"class SSIM(torch.nn.Module):\n    def __init__(self, window_size=11, size_average=True):\n        super(SSIM, self).__init__()\n        self.window_size = window_size\n        self.size_average = size_average\n        self.channel = 1\n        self.window = self.create_window(window_size, self.channel)\n\n    def gaussian(self, window_size, sigma):\n        gauss = torch.Tensor([np.exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n        return gauss/gauss.sum()\n\n    def create_window(self, window_size, channel):\n        _1D_window = self.gaussian(window_size, 1.5).unsqueeze(1)\n        _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n        window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n        return window\n\n    def _ssim(self, img1, img2, window, window_size, channel, size_average=True):\n        mu1 = F.conv2d(img1, window, padding=window_size//2, groups=channel)\n        mu2 = F.conv2d(img2, window, padding=window_size//2, groups=channel)\n\n        mu1_sq = mu1.pow(2)\n        mu2_sq = mu2.pow(2)\n        mu1_mu2 = mu1*mu2\n\n        sigma1_sq = F.conv2d(img1*img1, window, padding=window_size//2, groups=channel) - mu1_sq\n        sigma2_sq = F.conv2d(img2*img2, window, padding=window_size//2, groups=channel) - mu2_sq\n        sigma12 = F.conv2d(img1*img2, window, padding=window_size//2, groups=channel) - mu1_mu2\n\n        C1 = 0.01**2\n        C2 = 0.03**2\n\n        ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n\n        if size_average:\n            return ssim_map.mean()\n        else:\n            return ssim_map.mean(1).mean(1).mean(1)\n\n    def forward(self, img1, img2):\n        (_, channel, _, _) = img1.size()\n\n        if channel == self.channel and self.window.data.type() == img1.data.type():\n            window = self.window\n        else:\n            window = self.create_window(self.window_size, channel)\n\n            if img1.is_cuda:\n                window = window.cuda(img1.get_device())\n            window = window.type_as(img1)\n\n            self.window = window\n            self.channel = channel\n\n        return self._ssim(img1, img2, window, self.window_size, channel, self.size_average)\n","metadata":{"id":"ohFMA8azQgfp","trusted":true,"execution":{"iopub.status.busy":"2025-05-26T21:20:47.760249Z","iopub.execute_input":"2025-05-26T21:20:47.760506Z","iopub.status.idle":"2025-05-26T21:20:48.181297Z","shell.execute_reply.started":"2025-05-26T21:20:47.760490Z","shell.execute_reply":"2025-05-26T21:20:48.180589Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":"## Level Weighted SSIM (LWSSIM)","metadata":{"id":"gSqTSwHgQaz5"}},{"cell_type":"code","source":"#NEW!\nclass LWSSIM(nn.Module):\n    \"\"\"\n    Level Weighted Structural Similarity Loss with improved numerical stability\n    to prevent NaN errors during backpropagation.\n    \"\"\"\n    def __init__(self, filter_sizes=[11, 9, 7, 5, 3], filter_weights=None,\n                 data_range=1.0, alpha=1.0, beta=1.0, gamma=1.0,\n                 C1=0.01**2, C2=0.03**2, C3=None, eps=1e-8):\n        super(LWSSIM, self).__init__()\n\n        # Filter sizes for different scales\n        self.filter_sizes = filter_sizes\n        self.num_levels = len(filter_sizes)\n\n        # Default to equal weighting if not specified\n        if filter_weights is None:\n            self.filter_weights = torch.ones(self.num_levels) / self.num_levels\n        else:\n            self.filter_weights = torch.tensor(filter_weights)\n            # Normalize weights to sum to 1\n            self.filter_weights = self.filter_weights / self.filter_weights.sum()\n\n        self.register_buffer('weights', self.filter_weights)\n\n        # Component weights\n        self.alpha = alpha  # Luminance weight\n        self.beta = beta    # Contrast weight\n        self.gamma = gamma  # Structure weight\n\n        # Constants to avoid division by zero\n        self.C1 = C1\n        self.C2 = C2\n        self.C3 = C3 if C3 is not None else C2/2\n\n        # Epsilon for numerical stability\n        self.eps = eps\n\n        self.data_range = data_range\n\n        # Create and register Gaussian windows individually\n        for size in filter_sizes:\n            # Create Gaussian window and register it as a buffer with a unique name\n            window = self._create_gaussian_window(size)\n            self.register_buffer(f'window_{size}', window)\n\n    def _create_gaussian_window(self, window_size, sigma=1.5):\n        \"\"\"\n        Create a 2D Gaussian window for filtering.\n        \"\"\"\n        # Create a 1D Gaussian kernel\n        coords = torch.arange(window_size, dtype=torch.float)\n        coords -= window_size // 2\n\n        # Gaussian function\n        gauss = torch.exp(-(coords ** 2) / (2 * sigma ** 2))\n        gauss /= gauss.sum()\n\n        # Create 2D Gaussian kernel by outer product\n        kernel = gauss.unsqueeze(0) * gauss.unsqueeze(1)\n\n        # Normalize\n        kernel /= kernel.sum()\n\n        # Reshape to [1, 1, window_size, window_size] for conv2d operation\n        return kernel.unsqueeze(0).unsqueeze(0)\n\n    def _get_window(self, window_size):\n        \"\"\"Get the Gaussian window for the given size.\"\"\"\n        return getattr(self, f'window_{window_size}')\n\n    def _compute_ssim_components(self, x, y, window_size):\n        \"\"\"\n        Compute the luminance, contrast, and structure components of SSIM\n        with improved numerical stability.\n        \"\"\"\n        window = self._get_window(window_size)\n        padding = window_size // 2\n\n        # Expand window to match input channels\n        channel_window = window.expand(x.shape[1], 1, window_size, window_size)\n\n        # Calculate means for each channel\n        mu_x = F.conv2d(x, channel_window, padding=padding, groups=x.shape[1])\n        mu_y = F.conv2d(y, channel_window, padding=padding, groups=y.shape[1])\n\n        mu_x_sq = mu_x ** 2\n        mu_y_sq = mu_y ** 2\n        mu_xy = mu_x * mu_y\n\n        # Calculate variances and covariance\n        sigma_x_sq = F.conv2d(x ** 2, channel_window, padding=padding, groups=x.shape[1]) - mu_x_sq\n        sigma_y_sq = F.conv2d(y ** 2, channel_window, padding=padding, groups=y.shape[1]) - mu_y_sq\n        sigma_xy = F.conv2d(x * y, channel_window, padding=padding, groups=x.shape[1]) - mu_xy\n\n        # Apply a stronger clamp to prevent very small negative values from numerical errors\n        sigma_x_sq = torch.clamp(sigma_x_sq, min=self.eps)\n        sigma_y_sq = torch.clamp(sigma_y_sq, min=self.eps)\n\n        # Compute standard deviations - add eps inside the sqrt for stability\n        sigma_x = torch.sqrt(sigma_x_sq)\n        sigma_y = torch.sqrt(sigma_y_sq)\n\n        # Luminance comparison\n        l = (2 * mu_xy + self.C1) / (mu_x_sq + mu_y_sq + self.C1)\n\n        # Contrast comparison - use stabilized formulation\n        c = (2 * sigma_x * sigma_y + self.C2) / (sigma_x_sq + sigma_y_sq + self.C2)\n\n        # Structure comparison - add eps to denominator for stability\n        s_numerator = sigma_xy + self.C3\n        s_denominator = sigma_x * sigma_y + self.C3\n        s = s_numerator / s_denominator\n\n        return l, c, s\n\n    def _compute_lwssim_single_scale(self, x, y, window_size):\n        \"\"\"\n        Compute the Level-Weighted SSIM at a single scale.\n        \"\"\"\n        l, c, s = self._compute_ssim_components(x, y, window_size)\n\n        # Key difference from standard SSIM:\n        # Addition instead of multiplication between luminance and other components\n        lwssim = self.alpha * l + self.beta * c * s\n\n        return lwssim\n\n    def forward(self, x, y):\n        \"\"\"\n        Compute LWSSIM loss between input and target images.\n        \"\"\"\n        # Ensure inputs are in the appropriate range\n        if self.data_range != 1.0:\n            x = x / self.data_range\n            y = y / self.data_range\n\n        # Clamp values for stability\n        x = torch.clamp(x, min=0, max=1)\n        y = torch.clamp(y, min=0, max=1)\n\n        # Compute multi-scale LWSSIM\n        multi_scale_lwssim = 0.0\n\n        for i, window_size in enumerate(self.filter_sizes):\n            # Compute LWSSIM at current scale\n            lwssim_val = self._compute_lwssim_single_scale(x, y, window_size)\n\n            # Apply weight for this scale\n            weight = self.weights[i]\n            multi_scale_lwssim += weight * lwssim_val\n\n        # Average across spatial dimensions\n        multi_scale_lwssim = multi_scale_lwssim.mean([2, 3])\n\n        # Average across batch and channels\n        lwssim_score = multi_scale_lwssim.mean()\n\n        return lwssim_score\n\n","metadata":{"id":"xBvMgzhaQLqn","trusted":true,"execution":{"iopub.status.busy":"2025-05-26T21:20:48.182079Z","iopub.execute_input":"2025-05-26T21:20:48.182282Z","iopub.status.idle":"2025-05-26T21:20:48.207498Z","shell.execute_reply.started":"2025-05-26T21:20:48.182266Z","shell.execute_reply":"2025-05-26T21:20:48.206777Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":"## Per Region Overlap (PRO) Score","metadata":{"id":"13YXOzG_HTrS"}},{"cell_type":"code","source":"def calculate_pro_score(anomaly_maps, ground_truth_masks, num_thresholds=100, max_fpr=1):\n    \"\"\"\n    Optimized version of PRO calculation using vectorized operations.\n\n    Args:\n        anomaly_maps: List of anomaly score maps\n        ground_truth_masks: List of binary ground truth masks\n        num_thresholds: Number of thresholds to evaluate\n        max_fpr: Maximum false positive rate threshold\n\n    Returns:\n        pro_score: The PRO score\n        pro_curve: The PRO values at each FPR point (for plotting)\n    \"\"\"\n\n    # Flatten scores and labels\n    all_scores = []\n    all_labels = []\n\n    for score_map, gt_mask in zip(anomaly_maps, ground_truth_masks):\n        all_scores.append(score_map.flatten())\n        all_labels.append(gt_mask.flatten())\n\n    all_scores = np.concatenate(all_scores)\n    all_labels = np.concatenate(all_labels)\n\n    # Get thresholds from ROC curve\n    fpr, tpr, thresholds = roc_curve(all_labels, all_scores)\n\n    # Sample thresholds uniformly\n    if len(thresholds) > num_thresholds:\n        sampled_indices = np.linspace(0, len(thresholds) - 1, num_thresholds, dtype=int)\n        thresholds = thresholds[sampled_indices]\n        fpr = fpr[sampled_indices]\n\n    pro_curve = []\n\n    for i, threshold in enumerate(thresholds):\n        if fpr[i] > max_fpr:\n            break\n\n        overlaps = []\n\n        for score_map, gt_mask in zip(anomaly_maps, ground_truth_masks):\n            # Binary prediction\n            prediction = (score_map >= threshold).astype(np.uint8)\n\n            # Skip if no ground truth anomalies\n            if gt_mask.max() == 0:\n                continue\n\n            # Label connected components\n            labeled_gt, num_regions = measure.label(gt_mask, return_num=True, connectivity=2)\n\n            # Calculate overlap for each region\n            for region_id in range(1, num_regions + 1):\n                region_mask = (labeled_gt == region_id)\n\n                # Calculate overlap (IoU)\n                intersection = np.logical_and(prediction, region_mask).sum()\n                union = np.logical_or(prediction, region_mask).sum()\n\n                if union > 0:\n                    overlap = intersection / union\n                    overlaps.append(overlap)\n\n        # Average overlap at this threshold\n        if overlaps:\n            pro_curve.append(np.mean(overlaps))\n        else:\n            pro_curve.append(0.0)\n\n    # Integrate PRO curve up to max_fpr\n    if pro_curve:\n        pro_score = np.mean(pro_curve)\n    else:\n        pro_score = 0.0\n\n    return pro_score, np.array(pro_curve)\n","metadata":{"id":"Ks0x9N0rHUla","trusted":true,"execution":{"iopub.status.busy":"2025-05-26T21:20:48.208275Z","iopub.execute_input":"2025-05-26T21:20:48.208508Z","iopub.status.idle":"2025-05-26T21:20:48.233701Z","shell.execute_reply.started":"2025-05-26T21:20:48.208492Z","shell.execute_reply":"2025-05-26T21:20:48.232964Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":"# Network","metadata":{"id":"OV22JsogwITN"}},{"cell_type":"markdown","source":"## DyT","metadata":{"id":"z6E2cL6_GrKJ"}},{"cell_type":"code","source":"class DyT(nn.Module):\n    \"\"\" Dynamic Tanh Layer \"\"\"\n    def __init__(self, dim, init_a=DYT_INIT_A):\n        super().__init__()\n        self.alpha = nn.Parameter(torch.tensor(init_a, dtype=torch.float32)) #0.5 come il paper di Lecun\n        self.gamma = nn.Parameter(torch.ones(dim, dtype=torch.float32) * 0.9) #Come il paper di LeCun, ma gamma leggermente pi piccolo\n        self.beta = nn.Parameter(torch.zeros(dim, dtype=torch.float32)) #Come il paper di LeCun\n        # To ensure gamma and beta can be broadcasted correctly with (B, N, D) input\n        # they need to be shaped as (1, 1, D) or (D)\n        # PyTorch handles (D) broadcasting to (B,N,D) if op is like self.gamma * tensor\n\n    def forward(self, x):\n        # x shape: (batch_size, num_patches, embed_dim)\n        return self.gamma * torch.tanh(self.alpha * x) + self.beta","metadata":{"execution":{"iopub.status.busy":"2025-05-26T21:20:48.234460Z","iopub.execute_input":"2025-05-26T21:20:48.234672Z","iopub.status.idle":"2025-05-26T21:20:48.259711Z","shell.execute_reply.started":"2025-05-26T21:20:48.234657Z","shell.execute_reply":"2025-05-26T21:20:48.259033Z"},"id":"qjdsgl4uGrKJ","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"## Residual Connection","metadata":{"id":"PJVmal_qJU8M"}},{"cell_type":"code","source":"class Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x","metadata":{"id":"qQ4AniaJJa5t","trusted":true,"execution":{"iopub.status.busy":"2025-05-26T21:20:48.260496Z","iopub.execute_input":"2025-05-26T21:20:48.260710Z","iopub.status.idle":"2025-05-26T21:20:48.275737Z","shell.execute_reply.started":"2025-05-26T21:20:48.260695Z","shell.execute_reply":"2025-05-26T21:20:48.275060Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":"## PreNorm","metadata":{"id":"2b2h5vSaJcLu"}},{"cell_type":"code","source":"class PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        if USING_DYT:\n            self.norm = DyT(dim)\n        else:\n            self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        global TIME_IN_NORM\n        global NORM_CALLS\n\n\n        start_time = time.time()\n        res = self.fn(self.norm(x), **kwargs)\n        end_time = time.time()\n\n        used_time = end_time - start_time\n\n        TIME_IN_NORM += used_time\n        NORM_CALLS += 1\n\n        return res","metadata":{"id":"KHIChCVsJuJu","trusted":true,"execution":{"iopub.status.busy":"2025-05-26T21:20:48.276494Z","iopub.execute_input":"2025-05-26T21:20:48.276745Z","iopub.status.idle":"2025-05-26T21:20:48.293524Z","shell.execute_reply.started":"2025-05-26T21:20:48.276722Z","shell.execute_reply":"2025-05-26T21:20:48.292805Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"## FeedForward","metadata":{"id":"yE3OwzkuJucv"}},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Linear(hidden_dim, dim)\n        )\n    def forward(self, x):\n        return self.net(x)","metadata":{"id":"9GYkL-vvJvkV","trusted":true,"execution":{"iopub.status.busy":"2025-05-26T21:20:48.294136Z","iopub.execute_input":"2025-05-26T21:20:48.294297Z","iopub.status.idle":"2025-05-26T21:20:48.315103Z","shell.execute_reply.started":"2025-05-26T21:20:48.294284Z","shell.execute_reply":"2025-05-26T21:20:48.314213Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":"## Attention","metadata":{"id":"3NNjDLBIJwAO"}},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, dim, heads=8):\n        super().__init__()\n        self.heads = heads\n        self.scale = dim ** -0.5\n\n        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n        self.to_out = nn.Linear(dim, dim)\n\n    def forward(self, x, mask=None):\n        b, n, _, h = *x.shape, self.heads\n        qkv = self.to_qkv(x).chunk(3, dim=-1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), qkv)\n\n        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n        mask_value = -torch.finfo(dots.dtype).max\n\n        if mask is not None:\n            mask = F.pad(mask.flatten(1), (1, 0), value=True)\n            assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\n            mask = mask[:, None, :] * mask[:, :, None]\n            dots.masked_fill_(~mask, mask_value)\n            del mask\n\n        attn = dots.softmax(dim=-1)\n        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        out = self.to_out(out)\n        return out\n","metadata":{"id":"0VBo7QJwJxbV","trusted":true,"execution":{"iopub.status.busy":"2025-05-26T21:20:48.315839Z","iopub.execute_input":"2025-05-26T21:20:48.316046Z","iopub.status.idle":"2025-05-26T21:20:48.331064Z","shell.execute_reply.started":"2025-05-26T21:20:48.316029Z","shell.execute_reply":"2025-05-26T21:20:48.330396Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":"## Transformer","metadata":{"id":"VOfdB43YJ52Z"}},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, dim, depth, heads, mlp_dim):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Residual(PreNorm(dim, Attention(dim, heads=heads))),\n                Residual(PreNorm(dim, FeedForward(dim, mlp_dim)))\n            ]))\n\n    def forward(self, x, mask=None):\n        for attn, ff in self.layers:\n            x = attn(x, mask=mask)\n            x = ff(x)\n        return x","metadata":{"id":"UdMRHh3SJ7y5","trusted":true,"execution":{"iopub.status.busy":"2025-05-26T21:20:48.331852Z","iopub.execute_input":"2025-05-26T21:20:48.332029Z","iopub.status.idle":"2025-05-26T21:20:48.348572Z","shell.execute_reply.started":"2025-05-26T21:20:48.332016Z","shell.execute_reply":"2025-05-26T21:20:48.347879Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":"## ViT Encoder","metadata":{"id":"sAJV0EVTNVMm"}},{"cell_type":"code","source":"class ViTEncoder(nn.Module):\n    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels=3):\n        super().__init__()\n        assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n        num_patches = (image_size // patch_size) ** 2\n        patch_dim = channels * patch_size ** 2\n        assert num_patches > MIN_NUM_PATCHES, f'your number of patches ({num_patches}) is way too small for attention to be effective (at least 16). Try decreasing your patch size'\n\n        self.patch_size = patch_size\n\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n\n        self.transformer = Transformer(dim, depth, heads, mlp_dim)\n\n        self.to_cls_token = nn.Identity()\n\n    def forward(self, img, mask=None):\n        p = self.patch_size\n\n        x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n        x = self.patch_to_embedding(x)\n        b, n, _ = x.shape\n\n        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b=b)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x += self.pos_embedding[:, :(n + 1)]\n\n        x = self.transformer(x, mask)\n\n        x = self.to_cls_token(x[:, 1:, :])\n\n        return x","metadata":{"id":"SCx2vNJ2NVfm","trusted":true,"execution":{"iopub.status.busy":"2025-05-26T21:20:48.349277Z","iopub.execute_input":"2025-05-26T21:20:48.349519Z","iopub.status.idle":"2025-05-26T21:20:48.372258Z","shell.execute_reply.started":"2025-05-26T21:20:48.349496Z","shell.execute_reply":"2025-05-26T21:20:48.371454Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":"## DigitCaps","metadata":{"id":"sMxVPk1fNzA7"}},{"cell_type":"code","source":"class DigitCaps(nn.Module):\n    def __init__(self, out_num_caps=1, in_num_caps=8*8*64, in_dim_caps=8, out_dim_caps=512, decode_idx=-1):\n        super(DigitCaps, self).__init__()\n\n        self.in_dim_caps = in_dim_caps\n        self.in_num_caps = in_num_caps\n        self.out_dim_caps = out_dim_caps\n        self.out_num_caps = out_num_caps\n        self.decode_idx = decode_idx\n        self.W = nn.Parameter(0.01 * torch.randn(out_num_caps, in_num_caps, out_dim_caps, in_dim_caps))\n\n    def forward(self, x):\n        # x size: batch x 1152 x 8\n        x_hat = torch.squeeze(torch.matmul(self.W, x[:, None, :, :, None]), dim=-1)\n        x_hat_detached = x_hat.detach()\n        # x_hat size: batch x ndigits x 1152 x 16\n        b = Variable(torch.zeros(x.size(0), self.out_num_caps, self.in_num_caps)).to(DEVICE)\n        # b size: batch x ndigits x 1152\n\n        # Routing algorithm\n        num_iters = 3\n        for i in range(num_iters):\n            c = F.softmax(b, dim=1)\n            # c size: batch x ndigits x 1152\n            if i == num_iters - 1:\n                # output size: batch x ndigits x 1 x 16\n                outputs = self.squash(torch.sum(c[:, :, :, None] * x_hat, dim=-2, keepdim=True))\n            else:\n                outputs = self.squash(torch.sum(c[:, :, :, None] * x_hat_detached, dim=-2, keepdim=True))\n                b = b + torch.sum(outputs * x_hat_detached, dim=-1)\n\n        outputs = torch.squeeze(outputs, dim=-2)  # squeezing to remove ones at the dimension -1\n\n        # Choose the longest vector as the one to decode\n        if self.decode_idx == -1:\n            classes = torch.sqrt((outputs ** 2).sum(2))\n            classes = F.softmax(classes, dim=1)\n            _, max_length_indices = classes.max(dim=1)\n        else:  # always choose the same digitcaps\n            max_length_indices = torch.ones(outputs.size(0)).long() * self.decode_idx\n            max_length_indices.to(DEVICE)\n\n        masked = Variable(torch.sparse.torch.eye(self.out_num_caps)).to(DEVICE)\n\n        masked = masked.index_select(dim=0, index=max_length_indices)\n        t = (outputs * masked[:, :, None]).sum(dim=1).unsqueeze(1)\n\n        return t, outputs\n\n    def squash(self, input_tensor):\n        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n        output_tensor = squared_norm * input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n        return output_tensor","metadata":{"id":"J1GibK5rN1q-","trusted":true,"execution":{"iopub.status.busy":"2025-05-26T21:20:48.373096Z","iopub.execute_input":"2025-05-26T21:20:48.373555Z","iopub.status.idle":"2025-05-26T21:20:48.392062Z","shell.execute_reply.started":"2025-05-26T21:20:48.373533Z","shell.execute_reply":"2025-05-26T21:20:48.391466Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":"## Convolutional Decoder","metadata":{"id":"U8LQt5sKN7y4"}},{"cell_type":"code","source":"class ConvolutionalDecoder(nn.Module):\n    def __init__(self, in_channels):\n        super(ConvolutionalDecoder, self).__init__()\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(in_channels=in_channels, out_channels=16, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(16, affine=True),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(16, 32, 9, stride=3, padding=1),\n            nn.BatchNorm2d(32, affine=True),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(32, 32, 7, stride=5, padding=1),\n            nn.BatchNorm2d(32, affine=True),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(32, 16, 9, stride=2),\n            nn.BatchNorm2d(16, affine=True),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(16, 8, 6, stride=1),\n            nn.BatchNorm2d(8, affine=True),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(8, 3, 11, stride=1),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        recon = self.decoder(x)\n        return recon","metadata":{"id":"lpCsbAf9N7g7","trusted":true,"execution":{"iopub.status.busy":"2025-05-26T21:20:48.392773Z","iopub.execute_input":"2025-05-26T21:20:48.393000Z","iopub.status.idle":"2025-05-26T21:20:48.417388Z","shell.execute_reply.started":"2025-05-26T21:20:48.392983Z","shell.execute_reply":"2025-05-26T21:20:48.416897Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":"## VT AutoEncoder","metadata":{}},{"cell_type":"code","source":"class VT_AutoEncoder(nn.Module):\n    def __init__(self, image_size=512,\n                patch_size=64,\n                num_classes=1,\n                dim=512,\n                depth=6,\n                heads=8,\n                mlp_dim=1024,\n                train=True, use_mask=USING_RANDOM_MASK):\n\n        super(VT_AutoEncoder, self).__init__()\n        self.vt = ViTEncoder(\n            image_size=image_size,\n            patch_size=patch_size,\n            num_classes=num_classes,\n            dim=dim,\n            depth=depth,\n            heads=heads,\n            mlp_dim=mlp_dim)\n\n        self.decoder = ConvolutionalDecoder(8)\n        self.Digcap = DigitCaps(in_num_caps=((image_size//patch_size)**2)*8*8, in_dim_caps=8)\n        self.fixed_mask = torch.ones(1, image_size//patch_size, image_size//patch_size).bool().to(DEVICE)\n        self.Train = train\n\n        if self.Train:\n            print(\"\\nInitializing network weights...\")\n            initialize_weights(self.vt, self.decoder)\n\n    def forward(self, x):\n        b = x.size(0)\n        if self.Train:\n            gaussian_mask = torch.randn_like(self.fixed_mask.float()).to(DEVICE)\n            # Imposta True (1) dove il valore  maggiore di 0.4, altrimenti False (0)\n            mask = (gaussian_mask > 0.4).bool().to(DEVICE)\n            encoded = self.vt(x, mask)\n            encoded = self.add_noise(encoded, decaying=NOISE_DECAY)\n\n        else:\n            encoded = self.vt(x, self.fixed_mask)\n\n        encoded1, vectors = self.Digcap(encoded.view(b, encoded.size(1)*8*8, -1))\n        recons = self.decoder(encoded1.view(b, -1, 8, 8))\n\n        return encoded, recons\n\n    def add_noise(self, latent, sd=0.2, decaying=False):\n        global NOISE_DECAY_FACTOR\n        assert sd >= 0.0\n        n = torch.distributions.Normal(torch.tensor([0.]), torch.tensor([sd]))\n        noise = n.sample(latent.size()).squeeze(-1).to(DEVICE)\n\n        if decaying:\n            noise = noise * (1000/(1000 + NOISE_DECAY_FACTOR))\n            NOISE_DECAY_FACTOR += 1\n\n        latent = latent + noise\n        return latent","metadata":{"execution":{"iopub.status.busy":"2025-05-26T21:20:48.418080Z","iopub.execute_input":"2025-05-26T21:20:48.418313Z","iopub.status.idle":"2025-05-26T21:20:48.435892Z","shell.execute_reply.started":"2025-05-26T21:20:48.418293Z","shell.execute_reply":"2025-05-26T21:20:48.435356Z"},"id":"QDbc8RUkwITN","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":"## Mixture Density Network","metadata":{}},{"cell_type":"code","source":"\nclass MDN(nn.Module):\n    def __init__(self, input_dim=512, out_dim=512, layer_size=512, coefs=NUM_GAUSSIANS, test=False, sd=0.5):\n        super(MDN, self).__init__()\n        self.in_features = input_dim\n\n        self.pi = nn.Linear(layer_size, coefs, bias=False)\n        self.mu = nn.Linear(layer_size, out_dim * coefs, bias=False)  # mean\n        self.sigma_sq = nn.Linear(layer_size, out_dim * coefs, bias=False)  # isotropic independent variance\n        self.out_dim = out_dim\n        self.coefs = coefs\n        self.test = test\n        self.sd = sd\n\n    def forward(self, x):\n        ep = np.finfo(float).eps\n        x = torch.clamp(x, ep)\n\n        pi = F.softmax(self.pi(x), dim=-1)\n        sigma_sq = F.softplus(self.sigma_sq(x)).view(x.size(0), x.size(1), self.in_features, -1)  # logvar\n        mu = self.mu(x).view(x.size(0), x.size(1), self.in_features, -1)  # mean\n        return pi, mu, sigma_sq\n\n\n    def negative_log_likelihood(self, x, means, logvars, weights, test=False):\n\n        eps = 1e-8\n\n        x_squeezed = x.unsqueeze(-1).expand_as(logvars)\n        logvars = torch.clamp(logvars, min=-10, max=10)\n        a = (x_squeezed - means) ** 2\n        log_p = (logvars + a / (torch.exp(logvars) + eps)).sum(2)\n        log_p = -0.5 * (np.log(2 * np.pi) + log_p)\n\n        weighted_log_p = - weights * log_p\n        weighted_log_p = torch.sum(weighted_log_p, 2)\n\n        if test:\n            res = weighted_log_p\n        else:\n            res = torch.mean(torch.sum(weighted_log_p, 1))\n        return res","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T21:20:48.436540Z","iopub.execute_input":"2025-05-26T21:20:48.436788Z","iopub.status.idle":"2025-05-26T21:20:48.464085Z","shell.execute_reply.started":"2025-05-26T21:20:48.436766Z","shell.execute_reply":"2025-05-26T21:20:48.463608Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":"# Train","metadata":{"id":"ItL_3QR3wITO"}},{"cell_type":"code","source":"# Train\n########################\n\ndef train_model(save_best=False):\n    \"\"\"\n    Train the anomaly detection model\n\n    Args:\n        se safe_best=True salva il migliore ogni volta, altrimenti ogni 25 epochs.\n    \"\"\"\n    # we reset the global variables for time and calls for norm layer\n    global TIME_IN_NORM\n    global NORM_CALLS\n    TIME_IN_NORM = 0\n    NORM_CALLS = 0\n\n    print(f\"\\n{'='*20} Training on {DATASET_STR} dataset {'='*20}\")\n    print(f\"Product: {DATASET_CAT_STR}, Epochs: {NUM_EPOCHS}, Learning Rate: {LEARNING_RATE}\")\n\n    # Initialize wandb\n\n    # Initialize SSIM loss\n    if USING_LW_SSIM:\n        ssim = LWSSIM().to(DEVICE)\n    else:\n        ssim = SSIM().to(DEVICE)\n\n    if LOG_LPIPS:\n        lpips = LPIPS(net='vgg').to(DEVICE)\n    else:\n        lpips = None\n\n    # Load dataset\n    if DATASET_STR == 'MVTEC':\n        data = MVTEC(BATCH_SIZE, root=MVTEC_DATA_DIR, product=DATASET_CAT_STR)\n    elif DATASET_STR == 'BTAD':\n        data = BTAD(BATCH_SIZE, root=BTAD_DATA_DIR, product=DATASET_CAT_STR)\n    else:\n        raise ValueError(f\"Dataset {DATASET_STR} not supported\")\n\n    # Initialize models\n    model = VT_AutoEncoder(patch_size=PATCH_SIZE, train=True, use_mask=USING_RANDOM_MASK).to(DEVICE)\n    G_estimate = MDN().to(DEVICE)\n\n    # Initialize optimizer\n    optimizer = Adam(list(model.parameters()) + list(G_estimate.parameters()),\n                    lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n\n    # Set models to train mode\n    model.train()\n    G_estimate.train()\n\n    # Training loop\n    minloss = float('inf')\n    best_epoch = 0\n\n    print('\\nNetwork training started...')\n    start_time = time.time()\n    for epoch in range(NUM_EPOCHS):\n        epoch_losses = []\n        epoch_mse_losses = []\n        epoch_ssim_losses = []\n        epoch_mdn_losses = []\n        epoch_lpips_losses = []\n\n        # Both MVTec and BTAD objects have a .train_loader attribute\n        train_loader = data.train_loader\n\n        for images, masks in train_loader:\n            # Handle single channel images\n            if images.size(1) == 1:\n                images = torch.stack([images, images, images]).squeeze(2).permute(1, 0, 2, 3)\n\n            # Zero gradients\n            optimizer.zero_grad()\n\n            # Forward pass through models\n            vector, reconstructions = model(images.to(DEVICE))\n            pi, mu, sigma = G_estimate(vector)\n\n            # Calculate losses\n            mse_loss = F.mse_loss(reconstructions, images.to(DEVICE), reduction='mean')  # Reconstruction Loss\n            ssim_loss = -ssim(images.to(DEVICE), reconstructions)  # Structural similarity loss\n            mdn_loss = G_estimate.negative_log_likelihood(vector, mu, sigma, pi)  # Mixture density network loss\n\n            if LOG_LPIPS:\n                lpips_loss = lpips.forward(reconstructions, images.to(DEVICE)).mean()  # LPIPS loss\n            else:\n                lpips_loss = torch.tensor(0.0).to(DEVICE)\n\n            # Total loss\n            total_loss = LAMBDA_MSE * mse_loss + LAMBDA_SSIM * ssim_loss + LAMBDA_LPIPS * lpips_loss + mdn_loss\n\n            # Store loss\n            epoch_losses.append(total_loss.item())\n            epoch_mse_losses.append(mse_loss.item())\n            epoch_ssim_losses.append(ssim_loss.item())\n            epoch_mdn_losses.append(mdn_loss.item())\n            epoch_lpips_losses.append(lpips_loss.item())\n\n            # Backpropagate and update weights\n            total_loss.backward()\n            optimizer.step()\n\n        # Calculate epoch average loss\n        avg_epoch_loss = np.mean(epoch_losses)\n        avg_mse_loss = np.mean(epoch_mse_losses)\n        avg_ssim_loss = np.mean(epoch_ssim_losses)\n        avg_mdn_loss = np.mean(epoch_mdn_losses)\n        avg_lpips_loss = np.mean(epoch_lpips_losses)\n\n        # Log to wandb\n        wandb.log({\n            \"epoch\": epoch,\n            \"total_loss\": avg_epoch_loss,\n            \"mse_loss\": avg_mse_loss,\n            \"ssim_loss\": avg_ssim_loss,\n            \"mdn_loss\": avg_mdn_loss,\n            \"lpips_loss\": avg_lpips_loss\n        })\n\n        # Log reconstructed images\n        if epoch % 10 == 0:\n            # Get a sample batch for visualization\n            sample_images, _ = next(iter(train_loader))\n            if sample_images.size(1) == 1:\n                sample_images = torch.stack([sample_images, sample_images, sample_images]).squeeze(2).permute(1, 0, 2, 3)\n\n            with torch.no_grad():\n                _, sample_reconstructions = model(sample_images.to(DEVICE))\n\n            # Create comparison grid\n            comparison = torch.cat([sample_images[:4], sample_reconstructions[:4].cpu()])\n            grid = utils.make_grid(comparison, nrow=4)\n\n            # Log to tensorboard and wandb\n            wandb.log({\"reconstructions\": wandb.Image(grid)})\n\n        print(f'Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {avg_epoch_loss:.3f}')\n\n        # Save best model\n        if avg_epoch_loss < minloss and save_best:\n            minloss = avg_epoch_loss\n            best_epoch = epoch\n            os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n\n            torch.save(model.state_dict(), model_path)\n            torch.save(G_estimate.state_dict(), g_path)\n            print(f\"Saved best model at epoch {epoch+1} with loss {minloss:.3f}\")\n\n            # Save to wandb\n            if IS_KAGGLE or IS_COLAB:\n                wandb.save(model_path)\n                wandb.save(g_path)\n\n        elif (epoch % 25 == 0 and epoch != 0) or (epoch == NUM_EPOCHS - 1):\n            os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n            torch.save(model.state_dict(), model_path)\n            torch.save(G_estimate.state_dict(), g_path)\n            print(f\"Saved model at epoch {epoch+1} with loss {avg_epoch_loss:.3f}\")\n\n            # Save to wandb\n            if IS_KAGGLE or IS_COLAB:\n                wandb.save(model_path)\n                wandb.save(g_path)\n\n    end_time = time.time()\n    elapsed_time = end_time - start_time\n    print(f\"\\nTraining completed in {elapsed_time:.2f} seconds.\")\n    print(f\"Time per normalization {NORM_STR}: {TIME_IN_NORM:.8f} seconds\")\n    print(f\"Model saved at: {os.path.join(MODEL_SAVE_DIR, f'VT_AE_{DATASET_STR}_{DATASET_CAT_STR}.pt')}\")\n    print(f\"MDN model saved at: {os.path.join(MODEL_SAVE_DIR, f'G_estimate_{DATASET_STR}_{DATASET_CAT_STR}.pt')}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-05-26T21:20:48.464691Z","iopub.execute_input":"2025-05-26T21:20:48.464844Z","iopub.status.idle":"2025-05-26T21:20:48.494282Z","shell.execute_reply.started":"2025-05-26T21:20:48.464832Z","shell.execute_reply":"2025-05-26T21:20:48.493755Z"},"id":"m3EC7hITwITO","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":"# Evaluation","metadata":{"id":"ekQMpH49wITO"}},{"cell_type":"markdown","source":"## Image-Level Threshold","metadata":{"id":"SL9brAqGRsqX"}},{"cell_type":"code","source":"def calculate_image_level_threshold(model, G_estimate, test_norm_loader, test_anom_loader, patch_size, target_fpr=0.05):\n    \"\"\"\n    Calculate an image-level threshold based on np.max(score_map) for each image.\n\n    Args:\n        model: Trained VT_AE model.\n        G_estimate: Trained MDN model.\n        test_norm_loader: DataLoader for normal test/validation images.\n        test_anom_loader: DataLoader for anomalous test/validation images.\n        patch_size: Patch size used by the model.\n        target_fpr (float, optional): The desired maximum false positive rate at the image level.\n                                     If None, uses Youden's J statistic to find the threshold.\n                                     Defaults to 0.05.\n\n    Returns:\n        float: Calculated image-level threshold.\n    \"\"\"\n    model.eval()\n    G_estimate.eval()\n\n    image_max_scores = []\n    image_true_labels = [] # 0 for normal, 1 for anomaly\n\n    # Process normal images\n    print(\"Calculating threshold: Processing normal samples...\")\n    for images, _ in test_norm_loader: # Masks for normal are all zeros, not needed for label here\n        if images.size(1) == 1: # Handle single channel images\n            images = torch.stack([images, images, images]).squeeze(2).permute(1,0,2,3).to(DEVICE)\n\n        images = images.to(DEVICE)\n\n        with torch.no_grad():\n            vector, reconstructions = model(images)\n            pi, mu, sigma = G_estimate(vector)\n            mdn_loss = G_estimate.negative_log_likelihood(vector, mu, sigma, pi, test=True) # Shape (batch, num_patches)\n\n        mdn_loss_np = mdn_loss.detach().cpu().numpy()\n\n        # Upsampling layer (ensure it's on the correct device if created here, or pass as arg)\n        upsample_layer = torch.nn.UpsamplingBilinear2d((512, 512))\n\n        # Reshape to (batch_size, 1, H_patch, W_patch)\n        # Note: args.patch_size would be better if passed directly or from a config\n        h_patch = 512 // patch_size\n        w_patch = 512 // patch_size\n        norm_score_batch_reshaped = mdn_loss_np.reshape(-1, 1, h_patch, w_patch)\n\n        # Upsample\n        score_map_batch_upsampled = upsample_layer(torch.tensor(norm_score_batch_reshaped)) # (batch_size, 1, 512, 512)\n\n        for batch_idx in range(images.size(0)):\n            # Filter one image's score map: score_map_batch_upsampled is (B, C, H, W)\n            # Convert to numpy, select channel 0 for filtering\n            score_map_one_image_np = score_map_batch_upsampled[batch_idx, 0].cpu().numpy()\n            score_map_one_image_filtered = gaussian_filter(score_map_one_image_np, sigma=4) # Assuming Filter expects (H,W)\n\n            image_max_scores.append(np.max(score_map_one_image_filtered))\n            image_true_labels.append(0) # This is a normal image\n\n    # Process anomalous images\n    print(\"Calculating threshold: Processing anomalous samples...\")\n    for images, masks in test_anom_loader:\n        if images.size(1) == 1: # Handle single channel images\n            images = torch.stack([images, images, images]).squeeze(2).permute(1,0,2,3)\n\n        images = images.to(DEVICE)\n        masks = masks.to(DEVICE)\n\n        with torch.no_grad():\n            vector, reconstructions = model(images)\n            pi, mu, sigma = G_estimate(vector)\n            mdn_loss = G_estimate.negative_log_likelihood(vector, mu, sigma, pi, test=True)\n\n        mdn_loss_np = mdn_loss.detach().cpu().numpy()\n        upsample_layer = torch.nn.UpsamplingBilinear2d((512, 512))\n        h_patch = 512 // patch_size\n        w_patch = 512 // patch_size\n        norm_score_batch_reshaped = mdn_loss_np.reshape(-1, 1, h_patch, w_patch)\n        score_map_batch_upsampled = upsample_layer(torch.tensor(norm_score_batch_reshaped))\n\n        for batch_idx in range(images.size(0)):\n            score_map_one_image_np = score_map_batch_upsampled[batch_idx, 0].cpu().numpy()\n            score_map_one_image_filtered = gaussian_filter(score_map_one_image_np, sigma=4)\n            image_max_scores.append(np.max(score_map_one_image_filtered))\n\n            # Determine if the ground truth mask indicates an anomaly for this image\n            true_mask_for_image = masks[batch_idx, 0].cpu().numpy()\n            # An image from anom_loader is considered anomalous if its mask has any positive pixels\n            is_truly_anomalous = 1 if np.sum(true_mask_for_image) > 0 else 0\n            image_true_labels.append(is_truly_anomalous)\n\n\n    if not image_max_scores:\n        print(\"Warning: No scores collected for threshold calculation. Returning a default threshold (e.g., 0.5).\")\n        return 0.5\n\n    fpr, tpr, roc_thresholds = roc_curve(image_true_labels, image_max_scores)\n\n    chosen_threshold = 0.5 # Default\n\n    if target_fpr is not None:\n        # Find thresholds where actual_fpr <= target_fpr\n        candidate_indices = np.where(fpr <= target_fpr)[0]\n        if len(candidate_indices) > 0:\n            # Among these, pick the one with the highest TPR\n            best_candidate_idx = candidate_indices[np.argmax(tpr[candidate_indices])]\n            chosen_threshold = roc_thresholds[best_candidate_idx]\n            actual_fpr_val = fpr[best_candidate_idx]\n            actual_tpr_val = tpr[best_candidate_idx]\n            print(f\"Threshold (for image-level FPR <= {target_fpr}): {chosen_threshold:.4f} (Actual FPR: {actual_fpr_val:.4f}, TPR: {actual_tpr_val:.4f})\")\n        else:\n            # No threshold meets target_fpr, fall back to Youden's J or warn\n            print(f\"Warning: No threshold found for image-level target FPR <= {target_fpr}. Falling back to Youden's J.\")\n            if len(tpr) > 0 and len(fpr) > 0 : # Ensure not empty\n                best_idx = np.argmax(tpr - fpr) # Youden's J\n                chosen_threshold = roc_thresholds[best_idx]\n                print(f\"Threshold (Youden's J fallback): {chosen_threshold:.4f} (FPR: {fpr[best_idx]:.4f}, TPR: {tpr[best_idx]:.4f})\")\n            else:\n                print(\"Warning: ROC curve could not be computed properly. Using default threshold.\")\n    else: # If target_fpr is None, just use Youden's J\n        if len(tpr) > 0 and len(fpr) > 0 : # Ensure not empty\n            best_idx = np.argmax(tpr - fpr) # Youden's J\n            chosen_threshold = roc_thresholds[best_idx]\n            print(f\"Threshold (Youden's J): {chosen_threshold:.4f} (FPR: {fpr[best_idx]:.4f}, TPR: {tpr[best_idx]:.4f})\")\n        else:\n            print(\"Warning: ROC curve could not be computed properly. Using default threshold.\")\n\n    return chosen_threshold","metadata":{"id":"87gQzCj_Rsck","trusted":true,"execution":{"iopub.status.busy":"2025-05-26T21:20:48.494978Z","iopub.execute_input":"2025-05-26T21:20:48.495215Z","iopub.status.idle":"2025-05-26T21:20:48.520473Z","shell.execute_reply.started":"2025-05-26T21:20:48.495195Z","shell.execute_reply":"2025-05-26T21:20:48.519922Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":"## Threshold (credo pixel level)","metadata":{"id":"jRZfYyQZRyWr"}},{"cell_type":"code","source":"def calculate_threshold(model, G_estimate, data_loaders, patch_size, fpr_threshold=0.3):\n    \"\"\"\n    Calculate threshold for anomaly detection\n\n    Args:\n        model: Trained VT_AE model\n        G_estimate: Trained MDN model\n        data_loaders: List of data loaders to use for threshold calculation\n        patch_size: Patch size used in the model\n        fpr_threshold: False positive rate threshold\n\n    Returns:\n        threshold: Calculated threshold\n    \"\"\"\n    norm_loss_values = []\n    normalised_scores = []\n    mask_scores = []\n\n    for data_loader in data_loaders:\n        for images, masks in data_loader:\n            # Handle single channel images\n            if images.size(1) == 1:\n                images = torch.stack([images, images, images]).squeeze(2).permute(1, 0, 2, 3)\n\n            # Forward pass\n            vector, reconstructions = model(images.to(DEVICE))\n            pi, mu, sigma = G_estimate(vector)\n\n            # Calculate MDN loss (anomaly score)\n            mdn_loss = G_estimate.negative_log_likelihood(vector, mu, sigma, pi, test=True)\n            norm_loss_values.append(mdn_loss.detach().cpu().numpy())\n\n            # Process masks and scores for ROC calculation\n            m = torch.nn.UpsamplingBilinear2d((512, 512))\n            norm_score = norm_loss_values[-1].reshape(-1, 1, 512//patch_size, 512//patch_size)\n            score_map = m(torch.tensor(norm_score))\n            score_map = gaussian_filter(score_map, sigma=4)\n\n            mask_scores.append(masks.squeeze(0).squeeze(0).cpu().numpy())\n            normalised_scores.append(score_map)\n\n    # Flatten scores and masks\n    scores = np.asarray(normalised_scores).flatten()\n    masks = np.asarray(mask_scores).flatten()\n\n    # Calculate ROC curve and find threshold\n    fpr, tpr, thresholds = roc_curve(masks, scores)\n    fp_indices = np.where(fpr <= fpr_threshold)\n    threshold = thresholds[fp_indices[-1][-1]]\n\n    return threshold\n","metadata":{"id":"MmMqbzc4Rvq0","trusted":true,"execution":{"iopub.status.busy":"2025-05-26T21:20:48.521096Z","iopub.execute_input":"2025-05-26T21:20:48.521288Z","iopub.status.idle":"2025-05-26T21:20:48.543296Z","shell.execute_reply.started":"2025-05-26T21:20:48.521263Z","shell.execute_reply":"2025-05-26T21:20:48.542814Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":"## Evaluation function","metadata":{"id":"vRRI_AVwRvfH"}},{"cell_type":"code","source":"# Evaluation\n########################\n\ndef test_model():\n    \"\"\"\n    Test the anomaly detection model\n    \"\"\"\n    # we reset the global variables for time and calls for norm layer\n    global TIME_IN_NORM\n    global NORM_CALLS\n    TIME_IN_NORM = 0\n    NORM_CALLS = 0\n\n    print(f\"\\n{'='*20} Testing on {DATASET_STR} dataset {'='*20}\")\n    print(f\"Product: {DATASET_CAT_STR}, Patch Size: {PATCH_SIZE}\")\n\n    # Create results directory\n    results_dir = os.path.join(MODEL_SAVE_DIR, f\"results_{DATASET_STR}_{DATASET_CAT_STR}\")\n    os.makedirs(results_dir, exist_ok=True)\n\n    # TODO BRILLARE\n    # # Initialize SSIM loss\n    # if USING_LW_SSIM:\n    #     ssim = LWSSIM().to(DEVICE)\n    # else:\n    #     ssim = SSIM().to(DEVICE)\n\n    # Load dataset\n    if DATASET_STR == 'MVTEC':\n        data = MVTEC(1, root=MVTEC_DATA_DIR, product=DATASET_CAT_STR)\n    elif DATASET_STR == 'BTAD':\n        # For BTAD, we need different handling for test\n        data = BTAD(1, root=BTAD_DATA_DIR, product=DATASET_CAT_STR)\n    else:\n        raise ValueError(f\"Dataset {DATASET_STR} not supported\")\n\n    # Load models\n    model = VT_AutoEncoder(patch_size=PATCH_SIZE, train=False).to(DEVICE)\n    G_estimate = MDN().to(DEVICE)\n\n\n    try:\n        model.load_state_dict(torch.load(model_path))\n        G_estimate.load_state_dict(torch.load(g_path))\n        print(f\"Models loaded from {model_path} and {g_path}\")\n    except Exception as e:\n        print(f\"Error loading models: {e}\")\n        return\n\n    # Set models to eval mode\n    model.eval()\n    G_estimate.eval()\n\n    # Calculate threshold\n    print(\"Calculating threshold...\")\n    test_loaders = [data.test_norm_loader, data.test_anom_loader]\n    threshold = calculate_threshold(model, G_estimate, test_loaders, PATCH_SIZE)\n    print(f\"VINTAGE!!! Threshold: {threshold}\")\n    # Pass the specific normal and anomaly loaders\n    threshold = calculate_image_level_threshold(\n        model,\n        G_estimate,\n        data.test_norm_loader, # Pass the normal loader\n        data.test_anom_loader, # Pass the anomaly loader\n        PATCH_SIZE,\n        target_fpr=None #0.05        # Adjust target_fpr as needed (e.g., 0.05 or 0.1 for image-level)\n                              # Or pass None to use Youden's J statistic by default\n    )\n    # Log threshold to wandb\n    wandb.config.update({\"threshold\": threshold})\n\n    # Evaluate on test data\n    print(\"Evaluating on test data...\")\n    with torch.no_grad():\n        # Lists to store results\n        normal_losses = []\n        anomaly_losses = []\n        normal_scores = []\n        anomaly_scores = []\n\n        # NEW: Lists to store full pixel-level data\n        full_score_maps = []\n        full_masks = []\n\n        all_y_true = []\n        all_y_pred = []\n\n        start_time = time.time()\n        # Process normal test data\n        for images, masks in data.test_norm_loader:\n            if images.size(1) == 1:\n                images = torch.stack([images, images, images]).squeeze(2).permute(1, 0, 2, 3)\n\n            vector, reconstructions = model(images.to(DEVICE))\n            pi, mu, sigma = G_estimate(vector)\n\n            mdn_loss = G_estimate.negative_log_likelihood(vector, mu, sigma, pi, test=True)\n            normal_losses.append(mdn_loss.sum().item())\n\n            # Generate score map\n            m = torch.nn.UpsamplingBilinear2d((512, 512))\n            norm_score = mdn_loss.detach().cpu().numpy().reshape(-1, 1, 512//PATCH_SIZE, 512//PATCH_SIZE)\n            score_map = m(torch.tensor(norm_score))\n            score_map = gaussian_filter(score_map.numpy(), sigma=4)\n\n            # Store full score map and corresponding zero mask for normal samples\n            full_score_maps.append(score_map[0][0])\n            full_masks.append(np.zeros_like(score_map[0][0]))\n\n            score_val = np.max(score_map)\n            normal_scores.append(score_val)\n\n            # Binary prediction (0: normal, 1: anomaly)\n            all_y_true.append(0)  # Ground truth: normal\n            all_y_pred.append(1 if score_val > threshold else 0)  # Prediction based on threshold\n\n        # Process anomalous test data\n        for idx, (images, masks) in enumerate(data.test_anom_loader):\n            if images.size(1) == 1:\n                images = torch.stack([images, images, images]).squeeze(2).permute(1, 0, 2, 3)\n\n            vector, reconstructions = model(images.to(DEVICE))\n            pi, mu, sigma = G_estimate(vector)\n\n            mdn_loss = G_estimate.negative_log_likelihood(vector, mu, sigma, pi, test=True)\n            anomaly_losses.append(mdn_loss.sum().item())\n\n            # Generate score map\n            m = torch.nn.UpsamplingBilinear2d((512, 512))\n            norm_score = mdn_loss.detach().cpu().numpy().reshape(-1, 1, 512//PATCH_SIZE, 512//PATCH_SIZE)\n            score_map = m(torch.tensor(norm_score))\n            score_map = gaussian_filter(score_map.numpy(), sigma=4)\n\n            # NEW: Store full score map and corresponding mask\n            full_score_maps.append(score_map[0][0])\n            full_masks.append(masks.squeeze(0).squeeze(0).cpu().numpy())\n\n            score_val = np.max(score_map)\n            anomaly_scores.append(score_val)\n\n            # Binary prediction (0: normal, 1: anomaly)\n            all_y_true.append(1)  # Ground truth: anomaly\n            all_y_pred.append(1 if score_val > threshold else 0)  # Prediction based on threshold\n\n            # Visualize some results and save\n            if idx % 5 == 0:\n                # Enhanced visualization\n                save_path = os.path.join(results_dir, f\"anomaly_{idx}.png\")\n                plot_enhanced(images, masks, reconstructions.cpu(), score_map[0][0], threshold, save_path)\n\n                # Region visualization\n                region_path = os.path.join(results_dir, f\"anomaly_regions_{idx}.png\")\n                visualize_regions(images, score_map[0][0], threshold, min_area=100, save_path=region_path)\n\n                # Log to wandb\n                wandb.log({\n                    f\"anomaly_sample_{idx}\": wandb.Image(save_path),\n                    f\"anomaly_regions_{idx}\": wandb.Image(region_path)\n                })\n        end_time = time.time()\n        elapsed_time = end_time - start_time\n        print(f\"Evaluation completed in {elapsed_time:.2f} seconds.\")\n        print(f\"Total Time for normalization {NORM_STR}: {TIME_IN_NORM:.4f} seconds\")\n        print(f\"Time per normalization {NORM_STR}: {TIME_IN_NORM/NORM_CALLS:.8f} seconds\")\n        # Calculate metrics\n        print(\"\\nCalculating evaluation metrics...\")\n\n        # Convert to numpy arrays\n        y_true = np.array(all_y_true)\n        y_pred = np.array(all_y_pred)\n        roc_labels = np.concatenate((np.zeros(len(normal_losses)), np.ones(len(anomaly_losses))))\n        roc_scores = np.concatenate((normal_losses, anomaly_losses))\n\n        # Image-level ROC AUC\n        image_auc = roc_auc_score(roc_labels, roc_scores)\n\n        # Calculate pixel-level ROC AUC  and PRO score\n        # Flatten and concatenate all score maps and masks\n        pixel_scores = np.concatenate([s.flatten() for s in full_score_maps])\n        pixel_masks = np.concatenate([m.flatten() for m in full_masks])\n\n        # Now the dimensions will match\n        pixel_auc = roc_auc_score(pixel_masks, pixel_scores)\n        pro_score, _ = calculate_pro_score(anomaly_maps = full_score_maps, ground_truth_masks = full_masks)\n\n        # Precision-Recall AUC\n        img_precision, img_recall, _ = precision_recall_curve(roc_labels, roc_scores)\n        img_pr_auc = auc(img_recall, img_precision)\n\n        pixel_precision, pixel_recall, _ = precision_recall_curve(pixel_masks, pixel_scores)\n        pr_auc = auc(pixel_recall, pixel_precision)\n\n        # Additional metrics\n        f1 = f1_score(y_true, y_pred)\n        accuracy = accuracy_score(y_true, y_pred)\n        precision_score_val = precision_score(y_true, y_pred)\n        recall_score_val = recall_score(y_true, y_pred)\n\n        # Generate ROC curve\n        img_fpr, img_tpr, _ = roc_curve(roc_labels, roc_scores)\n        pixel_fpr, pixel_tpr, _ = roc_curve(pixel_masks, pixel_scores)\n\n        # Plot and save img ROC curve\n        img_roc_path = os.path.join(results_dir, \"img_roc_curve.png\")\n        plot_roc_curve(img_fpr, img_tpr, image_auc, color='darkorange', title='Image ROC Curve',save_path=img_roc_path)\n\n        # Plot and save pixel ROC curve\n        pixel_roc_path = os.path.join(results_dir, \"pixel_roc_curve.png\")\n        plot_roc_curve(pixel_fpr, pixel_tpr, pixel_auc, color='purple', title='Pixel ROC Curve', save_path=pixel_roc_path)\n\n        # Plot and save img PR curve\n        img_pr_path = os.path.join(results_dir, \"img_pr_curve.png\")\n        plot_precision_recall_curve(img_precision, img_recall, img_pr_auc, color='green', title='Image PR Curve', save_path=img_pr_path)\n\n        # Plot and save pixel PR curve\n        pixel_pr_path = os.path.join(results_dir, \"pixel_pr_curve.png\")\n        plot_precision_recall_curve(pixel_precision, pixel_recall, pr_auc, color='blue', title='Pixel PR Curve', save_path=pixel_pr_path)\n\n\n        # Plot and save confusion matrix\n        cm_path = os.path.join(results_dir, \"confusion_matrix.png\")\n        plot_confusion_matrix(y_true, y_pred, cm_path, normalize=True)\n\n        # Plot and save score distributions\n        dist_path = os.path.join(results_dir, \"score_distributions.png\")\n        plot_score_distributions(normal_scores, anomaly_scores, threshold, dist_path)\n\n        # Log results to wandb\n\n        wandb.summary[\"img_level_auc\"] = image_auc\n        wandb.summary[\"pixel_level_auc\"] = pixel_auc\n        wandb.summary[\"img_pr_auc\"] = img_pr_auc\n        wandb.summary[\"f1_score\"] = f1\n        wandb.summary[\"accuracy\"] = accuracy\n        wandb.summary[\"precision\"] = precision_score_val\n        wandb.summary[\"recall\"] = recall_score_val\n        wandb.summary[\"pro_score\"] = pro_score\n\n        wandb.log({\n            \"img_roc_curve\": wandb.Image(img_roc_path),\n            \"img_pr_curve\": wandb.Image(img_pr_path),\n            \"pixel_roc_curve\": wandb.Image(pixel_roc_path),\n            \"pixel_pr_curve\": wandb.Image(pixel_pr_path),\n            \"confusion_matrix\": wandb.Image(cm_path),\n            \"score_distributions\": wandb.Image(dist_path)\n        })\n\n        # Log classification report\n        cls_report = classification_report(y_true, y_pred, target_names=['Normal', 'Anomaly'])\n        with open(os.path.join(results_dir, \"classification_report.txt\"), \"w\") as f:\n            f.write(cls_report)\n\n        print(f\"\\nResults for {NORM_STR} - {DATASET_STR} - {DATASET_CAT_STR}:\")\n        print(f\"Per Region Overlap (PRO) Score: {pro_score:.4f}\")\n        print(f\"Image-level AUC: {image_auc:.4f}\")\n        print(f\"Pixel-level AUC [Real PRO]: {pixel_auc:.4f}\")\n        print(f\"Precision-Recall AUC: {pr_auc:.4f}\")\n        print(f\"F1 Score: {f1:.4f}\")\n        print(f\"Accuracy: {accuracy:.4f}\")\n        print(f\"Precision: {precision_score_val:.4f}\")\n        print(f\"Recall: {recall_score_val:.4f}\")\n        print(\"\\nClassification Report:\")\n        print(cls_report)\n\n    # Finish wandb\n    wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2025-05-26T21:20:48.544075Z","iopub.execute_input":"2025-05-26T21:20:48.544276Z","iopub.status.idle":"2025-05-26T21:20:48.570099Z","shell.execute_reply.started":"2025-05-26T21:20:48.544262Z","shell.execute_reply":"2025-05-26T21:20:48.569614Z"},"id":"gS-lK1IWwITO","trusted":true},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":"# Main","metadata":{"id":"9TzIeCq3wITO"}},{"cell_type":"code","source":"def main():\n\n    print(\"-\" * 30)\n    print(\"Paths Configuration:\")\n    print(f\"BASE_PATH: {BASE_PATH}\")\n    print(f\"MODEL_SAVE_DIR: {MODEL_SAVE_DIR}\")\n    print(f\"DATASET_DIR: {DATASET_DIR}\")\n    print(f\"MVTEC_DATA_DIR: {MVTEC_DATA_DIR}\")\n    print(f\"BTAD_DATA_DIR: {BTAD_DATA_DIR}\")\n\n    print(\"-\" * 30)\n    print(\"Run Configuration:\")\n    print(f\"Normalization: {NORM_STR}\")\n    print(f\"Dataset: {DATASET_STR}\")\n    print(f\"Category: {DATASET_CAT_STR}\")\n    print(f\"Epochs: {NUM_EPOCHS}\")\n    print(f\"Train Model: {TRAIN_MODEL}\")\n    print(f\"Compare Models: {COMPARE_MODELS}\")\n    print(f\"Using LW SSIM: {USING_LW_SSIM}\")\n    print(f\"Using Mask: {USING_RANDOM_MASK}\")\n    print(f\"Logging LPIPS: {LOG_LPIPS}\")\n    print(f\"Noise Decay: {NOISE_DECAY}\")\n    print(f\"Noise Decay Factor: {NOISE_DECAY_FACTOR}\")\n    print(\"-\" * 30)\n\n    # Setup wandb\n    setup_wandb()\n\n    # Train the model if specified\n    if TRAIN_MODEL:\n        train_model(save_best=False)\n\n    # Test the model\n    test_model()\n    wandb.finish()\n\n# https://wandb.ai/authorize?ref=models\nmain()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"execution":{"iopub.status.busy":"2025-05-26T21:20:48.572667Z","iopub.execute_input":"2025-05-26T21:20:48.572862Z"},"id":"_xrJRhdXwITO","outputId":"3b1b4208-9e81-49be-c062-6d6f0bfa328a","trusted":true},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"------------------------------\nPaths Configuration:\nBASE_PATH: /kaggle/working\nMODEL_SAVE_DIR: /kaggle/working/trained_models\nDATASET_DIR: /kaggle/input\nMVTEC_DATA_DIR: /kaggle/input/mvtec-ad\nBTAD_DATA_DIR: /kaggle/input/btad-beantech-anomaly-detection\n------------------------------\nRun Configuration:\nNormalization: DyT\nDataset: BTAD\nCategory: 01\nEpochs: 400\nTrain Model: True\nCompare Models: False\nUsing LW SSIM: True\nUsing Mask: True\nLogging LPIPS: False\nNoise Decay: True\nNoise Decay Factor: 1\n------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.11"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250526_212048-n2tnd3cb</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/CV_albe_gab_kri/CV_Final_runs/runs/n2tnd3cb' target=\"_blank\">DyT_BTAD_e400_lwssim_randommask_kaggle</a></strong> to <a href='https://wandb.ai/CV_albe_gab_kri/CV_Final_runs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/CV_albe_gab_kri/CV_Final_runs' target=\"_blank\">https://wandb.ai/CV_albe_gab_kri/CV_Final_runs</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/CV_albe_gab_kri/CV_Final_runs/runs/n2tnd3cb' target=\"_blank\">https://wandb.ai/CV_albe_gab_kri/CV_Final_runs/runs/n2tnd3cb</a>"},"metadata":{}},{"name":"stdout","text":"\n==================== Training on BTAD dataset ====================\nProduct: 01, Epochs: 400, Learning Rate: 0.0001\nLoading BTAD dataset for product 01...\nFound product '01' under nested directory 'BTech_Dataset_transformed'.\nFound 400 images for product 01 (train, ok=True, ko=False)\nFound product '01' under nested directory 'BTech_Dataset_transformed'.\nFound 21 images for product 01 (test, ok=True, ko=False)\nFound product '01' under nested directory 'BTech_Dataset_transformed'.\nFound 49 images for product 01 (test, ok=False, ko=True)\nBTAD dataset for product 01 loaded successfully!\nTrain loader: Ready\nTest normal loader: Ready\nTest anomalous loader: Ready\nValidation loader: Using test normal data\n\nInitializing network weights...\n\nNetwork training started...\nEpoch 1/400, Loss: 40898.105\n","output_type":"stream"}],"execution_count":null}]}